{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think Stats (v2) notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process for working with a dataset**\n",
    "\n",
    "- Importing and cleaning: Whatever format the data is in, it usually takes some time and effort to read the data, clean and transform it, and check that everything made it through the translation process intact.\n",
    "- Single variable explorations: I usually start by examining one variable at a time, finding out what the variables mean, looking at distributions of the values, and choosing appropriate summary statistics.\n",
    "- Pair-wise explorations: To identify possible relationships between variables, I look at tables and scatter plots, and compute correlations and linear fits.\n",
    "- Multivariate analysis: If there are apparent relationships between variables, I use multiple regression to add control variables and investigate more complex rela‐ tionships.\n",
    "- Estimation and hypothesis testing: When reporting statistical results, it is important to answer three questions: How big is the effect? How much variability should we expect if we run the same measurement again? Is it possible that the apparent effect is due to chance?\n",
    "- Visualization: During exploration, visualization is an important tool for finding possible relationships and effects. Then if an apparent effect holds up to scrutiny, visualization is an effective way to communicate results.\n",
    "\n",
    "\n",
    "**To address the limitations we use the tools of statistics**\n",
    "\n",
    "- Data collection - Use data from a large national survey with the goal of generating statistically valid inferences about the U.S population\n",
    "- Descriptive statistics - Generate statistics that summarize the data consisely and evaluate ways to visualise the data\n",
    "- Exploratory data analysis - look for patterns, differences and other features that address the question we are interested in. Check for inconsistencies and identify limitations\n",
    "- Estimation - We will use data from a sample to estimate characteristics of the general population\n",
    "- Hypothesis testing - Where we see apparant effects, like a difference between groups we will evaluate whether the effect might have happened by chance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Do first babies tend to arrive late? \n",
    "In many discussions people provide data to support their claims.**\n",
    "\n",
    "Anecdotal evidence is unpublished and usually personal to answer these questions. This fails because\n",
    "- Small number of observations (we have to capture natural variation)\n",
    "- Selection bias - people who join ths discussion join because their first babies were late\n",
    "- Confirmation bias - People who believe the claim could be more likely to contribute examples to confirm it. People who doubt more likely to cite counterexamples\n",
    "- Inaccuracy - personal stories could be misremembered, misrepresented or repeated inaccurately. \n",
    "\n",
    "**To address the limitations we use the tools of statistics**\n",
    "\n",
    "- Data collection - Use data from a large national survey with the goal of generating statistically valid inferences about the U.S population\n",
    "- Descriptive statistics - Generate statistics that summarize the data consisely and evaluate ways to visualise the data\n",
    "- Exploratory data analysis - look for patterns, differences and other features that address the question we are interested in. Check for inconsistencies and identify limitations\n",
    "- Estimation - We will use data from a sample to estimate characteristics of the general population\n",
    "- Hypothesis testing - Where we see apparant effects, like a difference between groups we will evaluate whether the effect might have happened by chance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: National Survey of family Growth:**\n",
    "- Cross sectional study - captures snapshot of a group in time. Most common alternative would be a longitudinal study which observes a group repeatedly over time.\n",
    "- Use data from Cycle 6 conducted Jan 2002 to March 2003\n",
    "- A sample is taken from the population. People who participate are called respondents\n",
    "- Cross sectional studies are meant to be representative, each member of the target population had equal chance of participating. \n",
    "- NSFG deliberately oversampled for Hispanics, African Americans and teenagers higher than their representation in the U.S population to make sure their groups are large enough to draw valid statistical inferences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Data\n",
    "- DataFrames (Using Pandas)\n",
    "- Variables - observe features\n",
    "- Transformation - cleaning\n",
    "- Validation - compare to published results\n",
    "- Interpretation - To work with data effectively, you have to think on two levels at the same time, the level of statistics and the level of context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe a variable by reporting values and how many times each value appears. This is the distribution of the variable. \n",
    "- A histogram is a common representation that shows the frequency of each value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 3: 3, 4: 2, 5: 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing Histograms - Summary statistics of interest\n",
    "- Central tendencies (mean, median, mode)\n",
    "- Mode (is there more than one cluster)\n",
    "- spread - how much variability is there in the values\n",
    "    - variance and standard deviation. use n-1 in the denominator when statistic is used to estimate the variance in a population using a sample. \n",
    "- Shape (gaussian distributions and uniform shape or skewed)\n",
    "- Tails of distribution \n",
    "- Outliers not always visible. Domain knowledge helps in this case. \n",
    "\n",
    "Pandas \n",
    "mean = live.prglngth.mean()\n",
    "var = live.prglngth.var()\n",
    "std = live.prglngth.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 1, 5: 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One method of creating a histogram can also use collections counters, or in pandas\n",
    "\n",
    "t = [1, 2, 2, 3, 5]\n",
    "\n",
    "hist = {}\n",
    "for x in t:\n",
    "    hist[x] = hist.get(x, 0) + 1\n",
    "    \n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect size\n",
    "\n",
    "- used to describe difference between two groups. One obvious choice is the difference in the means.\n",
    "- Another way to convey the size of the effect is to compare the difference between groups to the variability within groups.\n",
    "\n",
    "Cohens d statistic \n",
    "\n",
    "d = (x ̄1 - x ̄2 )/ s \n",
    "\n",
    "\n",
    "def CohenEffectSize(group1, group2):\n",
    "        diff = group1.mean() - group2.mean()\n",
    "        var1 = group1.var()\n",
    "        var2 = group2.var()\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "        d = diff / math.sqrt(pooled_var)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaper 3 Probability Mass Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to present a distribution is a PMF which maps from each value to it's probability.\n",
    "Probability is a frequency expressed as a fraction of the sample size, n.\n",
    "\n",
    "To get from frequencies to probabilities we divide through by n which is called normalization.\n",
    "\n",
    "Total probability = 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 0.2, 'two': 0.4, 'three': 0.2, 'five': 0.2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = {'one':1, 'two':2, 'three':1, 'five':1}\n",
    "n = sum(hist.values()) #sum of all the total freqencies\n",
    "d = {} #empty dictionary to fill\n",
    "\n",
    "for x, freq in hist.items():\n",
    "    d[x] = freq / n\n",
    "d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something like the class size paradox appears if you survey children and ask how many children are in their family. Families with many children are more likely to appear in your sample, and families with no children have no chance to be in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and PMFs are useful while you are exploring data and trying to identify patterns and relationships. Once you have an idea what is going on, a good next step is to design a visualization that makes the patterns you have identified as clear as possible.\n",
    "\n",
    "e.g In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and transform the data to emphasize differences:\n",
    "\n",
    "For now we should hold this conclusion only tentatively. We used the same dataset to identify an apparent difference and then chose a visualization that makes the difference apparent. We can’t be sure this effect is real; it might be due to random variation. We’ll address this concern later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas dataframe indexing - by column names, index names, index by column, row, loc, iloc, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Cumulative Distribution Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMFS are limited. For continuous variables we need to check a range.\n",
    "\n",
    "\n",
    "Percentiles\n",
    "If you have taken a standardized test, you probably got your results in the form of a raw score and a percentile rank. In this context, the percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile,” you did as well as or better than 90% of the people who took the exam.\n",
    "\n",
    "The CDF is a function of x, where x is any value that might appear in the distribution. To evaluate CDF(x) for a particular value of x, we compute the fraction of values in the distribution less than or equal to x.\n",
    "\n",
    "Total to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here’s what that looks like as a function that takes a sequence, t, and a value, x:\n",
    "def EvalCdf(t, x):\n",
    "    count = 0.0\n",
    "    for value in t:\n",
    "        if value <= x:\n",
    "            count += 1\n",
    "    prob = count / len(t)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [1, 2, 2, 3, 5] \n",
    "x = 3\n",
    "EvalCdf(t, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latex subscript:\n",
    "\n",
    "$x_{2}$\n",
    "\n",
    "Latex superscript:\n",
    "\n",
    "$x^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Modelling Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples.\n",
    "The alternative is an analytic distribution, which is characterized by a CDF that is a mathematical function. Analytic distributions can be used to model empirical distri‐ butions.\n",
    "\n",
    "**exponential distributions**\n",
    "\n",
    "CDF(x) = 1 - $e^{-λx}$\n",
    "\n",
    "If you plot the complementary CDF (CCDF) of a dataset that you think is exponential, you expect to see a function like:\n",
    "\n",
    "y ≈ $e^{-λx}$\n",
    "\n",
    "Taking the log of both sides yields:\n",
    "\n",
    "log y ≈ - λx\n",
    "\n",
    "So on a log-y scale the CCDF is a straight line with slope -λ. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter, λ, can be interpreted as a rate; that is, the number of events that occur, on average, in a unit of time. In this example, 44 babies are born in 24 hours, so the rate is λ = 0.0306 births per minute. The mean of an exponential distribution is 1 / λ, so the mean time between births is 32.7 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distributions (gaussian)\n",
    "- common because it describes many phenomena (CLT is a good reason why this is the case)\n",
    "- Standard normal distribution has  μ = 0 and σ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal Distribution\n",
    "\n",
    "CDFlognormal (x) = CDFnormal ( log x)\n",
    "The parameters of the lognormal distribution are usually denoted μ and σ. But remem‐ ber that these parameters are not the mean and standard deviation; the mean of a log‐ normal distribution is exp (μ + σ 2 / 2) and the standard deviation is ugly (see Wikipe‐ dia).\n",
    "\n",
    "The Pareto Distribution\n",
    "\n",
    "The Pareto distribution is named after the economist Vilfredo Pareto, who used it to describe the distribution of wealth. Since then, it has been used to describe phenomena in the natural and social sciences including sizes of cities and towns, sand particles and meteorites, and forest fires and earthquakes.\n",
    "\n",
    "Why Model?\n",
    "At the beginning of this chapter, I said that many real world phenomena can be modeled with analytic distributions. “So,” you might ask, “what?”\n",
    "Like all models, analytic distributions are abstractions, which means they leave out details that are considered irrelevant. For example, an observed distribution might have measurement errors or quirks that are specific to the sample; analytic models smooth out these idiosyncrasies.\n",
    "Analytic models are also a form of data compression. When a model fits a dataset well, a small set of parameters can summarize a large amount of data.\n",
    "\n",
    "But it is important to remember that all models are imperfect. Data from the real world never fit an analytic distribution perfectly. People sometimes talk as if data are generated by models; for example, they might say that the distribution of human heights is normal, or the distribution of income is lognormal. Taken literally, these claims cannot be true; there are always differences between the real world and mathematical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6 Probability Density Functions \n",
    "\n",
    "The derivative of a CDF is called a probability density function, or PDF. For example, the PDF of an exponential distribution is\n",
    "\n",
    "Evaluating a PDF for a particular value of x is usually not useful. The result is not a probability; it is a probability density.\n",
    "In physics, density is mass per unit of volume; in order to get a mass, you have to multiply by volume or, if the density is not constant, you have to integrate over volume.\n",
    "Similarly, probability density measures probability per unit of x. In order to get a probability mass, you have to integrate over x.\n",
    "\n",
    "Kernel density estimation (KDE) is an algorithm that takes a sample and finds an appropriately smooth PDF that fits the data. \n",
    "\n",
    "Estimating a density function with KDE is useful for several purposes:\n",
    "Visualization\n",
    "During the exploration phase of a project, CDFs are usually the best visualization of a distribution. After you look at a CDF, you can decide whether an estimated PDF is an appropriate model of the distribution. If so, it can be a better choice for presenting the distribution to an audience that is unfamiliar with CDFs.\n",
    "Interpolation\n",
    "An estimated PDF is a way to get from a sample to a model of the population. If you have reason to believe that the population distribution is smooth, you can use KDE to interpolate the density for values that don’t appear in the sample.\n",
    "Simulation\n",
    "Simulations are often based on the distribution of a sample. If the sample size is small, it might be appropriate to smooth the sample distribution using KDE, which allows the simulation to explore more possible outcomes, rather than replicating the observed data.\n",
    "\n",
    "Probability density function (PDF)\n",
    "The derivative of a continuous CDF, a function that maps a value to its probability density.\n",
    "Probability density\n",
    "A quantity that can be integrated over a range of values to yield a probability. If the values are in units of cm, for example, probability density is in units of probability per cm.\n",
    "Kernel density estimation (KDE)\n",
    "An algorithm that estimates a PDF based on a sample.\n",
    "discretize\n",
    "To approximate a continuous function or distribution with a discrete function. The opposite of smoothing.\n",
    "raw moment\n",
    "A statistic based on the sum of data raised to a power.\n",
    "central moment\n",
    "A statistic based on deviation from the mean, raised to a power.\n",
    "standardized moment\n",
    "A ratio of moments that has no units.\n",
    "skewness\n",
    "A measure of how asymmetric a distribution is.\n",
    "sample skewness\n",
    "A moment-based statistic intended to quantify the skewness of a distribution.\n",
    "Pearson’s median skewness coefficient\n",
    "A statistic intended to quantify the skewness of a distribution based on the median, mean, and standard deviation.\n",
    "robust\n",
    "A statistic is robust if it is relatively immune to the effect of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Relationships between Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two variables are related if knowing one gives you information about the other. For example, height and weight are related; people who are taller tend to be heavier. Of course, it is not a perfect relationship: there are short heavy people and tall light ones. But if you are trying to guess someone’s weight, you will be more accurate if you know their height than if you don’t.\n",
    "\n",
    "- Scatter plots \n",
    "\n",
    "We can’t get that information back, but we can minimize the effect on the scatter plot by jittering the data, which means adding random noise to reverse the effect of rounding off. Since these measurements were rounded to the nearest inch, they might be off by up to 0.5 inches or 1.3 cm. Similarly, the weights might be off by 0.5 kg.\n",
    "\n",
    "heights = thinkstats2.Jitter(heights, 1.3)\n",
    "        weights = thinkstats2.Jitter(weights, 0.5)\n",
    "Here’s the implementation of Jitter:\n",
    "    def Jitter(values, jitter=0.5):\n",
    "        n = len(values)\n",
    "        return np.random.uniform(-jitter, +jitter, n) + values\n",
    "        \n",
    "Figure 7-1 (right) shows the result. Jittering reduces the visual effect of rounding and makes the shape of the relationship clearer. But in general you should only jitter data for purposes of visualization and avoid using jittered data for analysis.\n",
    "\n",
    "**Characterising relationships**\n",
    "- e.g bin one variable and plot percentiles of another\n",
    "groupBY for instance \n",
    "\n",
    "**Correlation:**\n",
    "A correlation is a statistic intended to quantify the strength of the relationship between two variables.\n",
    "A challenge in measuring correlation is that the variables we want to compare are often not expressed in the same units. And even if they are in the same units, they come from different distributions.\n",
    "There are two common solutions to these problems:\n",
    "• Transform each value to a standard scores, which is the number of standard devi‐ ations from the mean. This transform leads to the “Pearson product-moment cor‐ relation coefficient.”\n",
    "• Transform each value to its rank, which is its index in the sorted list of values. This transform leads to the “Spearman rank correlation coefficient.”\n",
    "\n",
    "If X is a series of n values, xi, we can convert to standard scores by subtracting the mean and dividing by the standard deviation: zi = (xi - μ) / σ.\n",
    "The numerator is a deviation: the distance from the mean. Dividing by σ standard‐ izes the deviation, so the values of Z are dimensionless (no units) and their distribution has mean 0 and variance 1.\n",
    "If X is normally distributed, so is Z. But if X is skewed or has outliers, so does Z; in those cases, it is more robust to use percentile ranks. If we compute a new variable, R, so that\n",
    " Correlation | 83\n",
    "ri is the rank of xi, the distribution of R is uniform from 1 to n, regardless of the distri‐ bution of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance**\n",
    "is a measure of the tendency of two variables to vary together. If we have two series, X and Y, their deviations from the mean are\n",
    "\n",
    "dxi = xi - xbar\n",
    "dyi = yi - ybar\n",
    "\n",
    "where xbar is the sample mean of X and ybar is the sample mean of Y. If X and Y vary together, their deviations tend to have the same sign.\n",
    "\n",
    "If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together.\n",
    "Covariance is the mean of these products:\n",
    "C o v ( X , Y ) = n1 ∑dxi * dyi\n",
    "\n",
    "where n is the length of the two series (they have to be the same length).\n",
    "If you have studied linear algebra, you might recognize that Cov is the dot product of the deviations, divided by their length. So the covariance is maximized if the two vectors are identical, 0 if they are orthogonal, and negative if they point in opposite directions. thinkstats2 uses np.dot to implement Cov efficiently:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cov(xs, ys, meanx=None, meany=None):\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "    if meanx is None:\n",
    "        meanx = np.mean(xs)\n",
    "    if meany is None:\n",
    "        meany = np.mean(ys)\n",
    "    cov = np.dot(xs-meanx, ys-meany) / len(xs)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pearson’s Correlation\n",
    "Covariance is useful in some computations, but it is seldom reported as a summary statistic because it is hard to interpret. Among other problems, its units are the product of the units of X and Y. For example, the covariance of weight and height in the BRFSS dataset is 113 kilogram-centimeters, whatever that means.\n",
    "One solution to this problem is to divide the deviations by the standard deviation, which yields standard scores, and compute the product of standard scores:\n",
    "p = ( x i - x ̄ ) ( y i - y ̄ ) i SX SY\n",
    "Where SX and SY are the standard deviations of X and Y. The mean of these products is ρ = n1 ∑ p i\n",
    "Or we can rewrite ρ by factoring out SX and SY:\n",
    "ρ = Cov(X, Y) SX SY\n",
    "This value is called Pearson’s correlation after Karl Pearson, an influential early statis‐ tician. It is easy to compute and easy to interpret. Because standard scores are dimen‐ sionless, so is ρ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corr(xs, ys):\n",
    "        xs = np.asarray(xs)\n",
    "        ys = np.asarray(ys)\n",
    "        meanx, varx = MeanVar(xs)\n",
    "        meany, vary = MeanVar(ys)\n",
    "        corr = Cov(xs, ys, meanx, meany) / math.sqrt(varx * vary)\n",
    "        return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson’s correlation** is always between -1 and +1 (including both). If ρ is positive, we say that the correlation is positive, which means that when one variable is high, the other tends to be high. If ρ is negative, the correlation is negative, so when one variable is high, the other is low.\n",
    "     Pearson’s Correlation | 85\n",
    "The magnitude of ρ indicates the strength of the correlation. If ρ is 1 or -1, the variables are perfectly correlated, which means that if you know one, you can make a perfect prediction about the other.\n",
    "Most correlation in the real world is not perfect, but it is still useful. The correlation of height and weight is 0.51, which is a strong correlation compared to similar human- related variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nonlinear Relationships**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If Pearson’s correlation is near 0, it is tempting to conclude that there is no relationship between the variables, but that conclusion is not valid. Pearson’s correlation only meas‐ ures linear relationships. If there’s a nonlinear relationship, ρ understates its strength.\n",
    "\n",
    "The top row shows linear relationships with a range of correlations; you can use this row to get a sense of what different values of ρ look like. The second row shows perfect correlations with a range of slopes, which demonstrates that correlation is unrelated to slope (we’ll talk about estimating slope soon). The third row shows variables that are clearly related, but because the relationship is non-linear, the correlation coefficient is 0.\n",
    "The moral of this story is that you should always look at a scatter plot of your data before blindly computing a correlation coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spearman’s Rank Correlation**\n",
    "Pearson’s correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of outliers. Spearman’s rank correlation is an alternative that mitigates the effect of outliers and skewed distri‐ butions. To compute Spearman’s correlation, we have to compute the rank of each value, which is its index in the sorted sample. For example, in the sample [1, 2, 5, 7] the rank of the value 5 is 3, because it appears third in the sorted list. Then we compute Pearson’s correlation for the ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orrelation and Causation\n",
    "If variables A and B are correlated, there are three possible explanations: A causes B, or B causes A, or some other set of factors causes both A and B. These explanations are called “causal relationships”.\n",
    "Correlation alone does not distinguish between these explanations, so it does not tell you which ones are true. This rule is often summarized with the phrase “Correlation does not imply causation,”\n",
    "\n",
    "You could use a randomized controlled trial with a control and treatment group as one way to test for causation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g We can use the sample mean xbar to estimate the population parameter mu. If there are no outliers the sample mean minimises the MSE (Mean squared error) M S E = 1/m ∑ $( x ̄ - μ )^2$. Where m is the number of times you play the estimation game, not to be confused with\n",
    "n, which is the size of the sample used to compute x ̄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- maximum likelihood estimator (MLE).\n",
    "\n",
    "- guessing the variance - For large samples, S2 is an adequate estimator, but for small samples it tends to be too low. Because of this unfortunate property, it is called a biased estimator. An estimator is unbiased if the expected total (or mean) error, after many iterations of the estimation game, is 0. Then divide by n-1 for an unbiased estimator. \n",
    "\n",
    "- sampling distributions - e.g If repeated how would the estimated mean vary \n",
    "\n",
    "There are two common ways to summarize the sampling distribution:\n",
    "Standard error (SE)\n",
    "A measure of how far we expect the estimate to be off, on average. For each simulated experiment, we compute the error, x ̄ - μ, and then compute the root mean squared error (RMSE). In this example, it is roughly 2.5 kg.\n",
    "Confidence interval (CI)\n",
    "A range that includes a given fraction of the sampling distribution. For example, the 90% confidence interval is the range from the 5th to the 95th percentile. In this example, the 90% CI is (86, 94) kg.\n",
    "\n",
    "\n",
    "People often confuse standard error and standard deviation. Remember that stan‐ dard deviation describes variability in a measured quantity; in this example, the standard deviation of gorilla weight is 7.5 kg. Standard error describes variability in an estimate. In this example, the standard error of the mean, based on a sample of 9 measurements, is 2.5 kg.\n",
    "One way to remember the difference is that, as sample size increases, standard error gets smaller; standard deviation does not.\n",
    "\n",
    "People often think that there is a 90% probability that the actual parameter, μ, falls in the 90% confidence interval. Sadly, that is not true. If you want to make a claim like that, you have to use Bayesian methods (see my book, Think Bayes).\n",
    "\n",
    "Sampling Bias: - e.g a random telephone questionnaire, would be limited to only the people who are in the phone book. \n",
    "Measurement error and innacurcies of self reporting. \n",
    "\n",
    "**Glossary**\n",
    "- estimation\n",
    "The process of inferring the parameters of a distribution from a sample.\n",
    "- estimator\n",
    "A statistic used to estimate a parameter.\n",
    "- mean squared error (MSE)\n",
    "A measure of estimation error.\n",
    "- root mean squared error (RMSE)\n",
    "The square root of MSE, a more meaningful representation of typical error mag‐ nitude.\n",
    "- maximum likelihood estimator (MLE)\n",
    "An estimator that computes the point estimate most likely to be correct.\n",
    "- bias (of an estimator)\n",
    "The tendency of an estimator to be above or below the actual value of the parameter, when averaged over repeated experiments.\n",
    "- sampling error\n",
    "Error in an estimate due to the limited size of the sample and variation due to chance.\n",
    "- sampling bias\n",
    "Error in an estimate due to a sampling process that is not representative of the population.\n",
    "- measurement error\n",
    "Error in an estimate due to inaccuracy collecting or recording data.\n",
    "- sampling distribution\n",
    "The distribution of a statistic if an experiment is repeated many times.\n",
    "- standard error\n",
    "The RMSE of an estimate, which quantifies variability due to sampling error (but not other sources of error).\n",
    "- confidence interval\n",
    "An interval that represents the expected range of an estimator if an experiment is repeated many times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are the affects likely to appear in the larger population? Or might it appear in the sample by chance?\n",
    "\n",
    "“Given a sample and an apparent effect, what is the probability of seeing such an effect by chance?”\n",
    "\n",
    "- Quantify size of apparant size of the apparant effect by chosing a test statistic. e.g difference between means between the groups\n",
    "- define a null hypothesis, assuming the apparant effect is not real (no change or difference between groups)\n",
    "- compute a p-value, probability of seeing the apparant effect if the null hypothesis is true. compute actual difference in means then probability of seeing a difference as big or bigger under the null hypothesis\n",
    "- interpret the result, if p is low effect is statistically significant, unlikely to occur by chance. In that case we infer the effect is more likely to appear in the larger population. \n",
    "\n",
    "\n",
    "5% is the conventional p value threshold. Though depends on choice of test statistics and model of the null hypothesis.\n",
    "\n",
    "one sided vs two sided tests (half the p value). depending on shape of the distribution.\n",
    "\n",
    "could also a test a correlation using pearsons correlation as the test statistic or spearmans rank correlaation. \n",
    "null: no correlation. \n",
    "    \n",
    "Chi squared tests - In the previous section we used total deviation as the test statistic. But for testing pro‐ portions it is more common to use the chi-squared statistic:\n",
    "    \n",
    "**Errors**\n",
    "In classical hypothesis testing, an effect is considered statistically significant if the p- value is below some threshold, commonly 5%. This procedure raises two questions:\n",
    "• If the effect is actually due to chance, what is the probability that we will wrongly consider it significant? This probability is the false positive rate.\n",
    "• If the effect is real, what is the chance that the hypothesis test will fail? This prob‐ ability is the false negative rate.\n",
    "\n",
    "The false positive rate is relatively easy to compute: if the threshold is 5%, the false positive rate is 5%. Here’s why:\n",
    "\n",
    " If there is no real effect, the null hypothesis is true, so we can compute the distri‐ bution of the test statistic by simulating the null hypothesis. Call this distribution CDFT.\n",
    "• Each time we run an experiment, we get a test statistic, t, which is drawn from CDFT. Then we compute a p-value, which is the probability that a random value from CDFT exceeds t, so that’s 1 - CDFT (t).\n",
    "• The p-value is less than 5% if CDFT (t) is greater than 95%; that is, if t exceeds the 95th percentile. And how often does a value chosen from CDFT exceed the 95th percentile? 5% of the time.\n",
    "So if you perform one hypothesis test with a 5% threshold, you expect a false positive 1 time in 20.\n",
    "\n",
    "\n",
    "**Power**\n",
    "\n",
    "This result is often presented the other way around: if the actual difference is 0.78 weeks, we should expect a positive test only 30% of the time. This “correct positive rate” is called the power of the test, or sometimes “sensitivity.” It reflects the ability of the test to detect an effect of a given size.\n",
    "\n",
    "In this example, the test had only a 30% chance of yielding a positive result (again, assuming that the difference is 0.78 weeks). As a rule of thumb, a power of 80% is considered acceptable, so we would say that this test was “underpowered.”\n",
    "In general a negative hypothesis test does not imply that there is no difference between the groups; instead it suggests that if there is a difference, it is too small to detect with this sample size.\n",
    "\n",
    "**Replication**\n",
    "The hypothesis testing process I demonstrated in this chapter is not, strictly speaking, good practice.\n",
    "First, I performed multiple tests. If you run one hypothesis test, the chance of a false positive is about 1 in 20, which might be acceptable. But if you run 20 tests, you should expect at least one false positive, most of the time.\n",
    "Second, I used the same dataset for exploration and testing. If you explore a large dataset, find a surprising effect, and then test whether it is significant, you have a good chance of generating a false positive.\n",
    "To compensate for multiple tests, you can adjust the p-value threshold (see this Wiki‐ pedia page). Or you can address both problems by partitioning the data, using one set for exploration and the other for testing.\n",
    "In some fields these practices are required or at least encouraged. But it is also common to address these problems implicitly by replicating published results. Typically the first paper to report a new result is considered exploratory. Subsequent papers that replicate the result with new data are considered confirmatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 Linear Least Squares\n",
    "\n",
    "- Correlation coefficients measure the strength and sign of a relationship, but not the slope. There are several ways to estimate the slope; the most common is a linear least squares fit. A “linear fit” is a line intended to model the relationship between variables. A “least squares” fit is one that minimizes the mean squared error (MSE) between the line and the data.\n",
    "\n",
    "The vertical deviation from the line, or residual, is\n",
    "    res = ys - (inter + slope * xs)\n",
    "\n",
    "The residuals might be due to random factors like measurement error, or nonrandom factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, exercise, and body type.\n",
    "If we get the parameters inter and slope wrong, the residuals get bigger, so it makes intuitive sense that the parameters we want are the ones that minimize the residuals.\n",
    "We might try to minimize the absolute value of the residuals, or their squares, or their cubes; but the most common choice is to minimize the sum of squared residuals, sum(res^2)).\n",
    "Why? There are three good reasons and one less important one:\n",
    "CHAPTER 10 Linear Least Squares\n",
    "  117\n",
    "• Squaring has the feature of treating positive and negative residuals the same, which is usually what we want.\n",
    "• Squaring gives more weight to large residuals, but not so much weight that the largest residual always dominates.\n",
    "• If the residuals are uncorrelated and normally distributed with mean 0 and constant (but unknown) variance, then the least squares fit is also the maximum likelihood estimator of inter and slope. See Wikipedia.\n",
    "• The values of inter and slope that minimize the squared residuals can be computed efficiently.\n",
    "\n",
    "**Goodness of Fit**\n",
    "There are several ways to measure the quality of a linear model, or goodness of fit. One of the simplest is the standard deviation of the residuals.\n",
    "If you use a linear model to make predictions, Std(res) is the root mean squared error (RMSE) of your predictions.\n",
    "\n",
    "Another way to measure goodness of fit is the coefficient of determination, usually denoted R2 and called “R-squared”:\n",
    "\n",
    "def CoefDetermination(ys, res):\n",
    "        return 1 - Var(res) / Var(ys)\n",
    "Var(res) is the MSE of your guesses using the model, Var(ys) is the MSE without it. So their ratio is the fraction of MSE that remains if you use the model, and R2 is the fraction of MSE the model eliminates.\n",
    "For birth weight and mother’s age, R2 is 0.0047, which means that mother’s age predicts about half of 1% of variance in birth weight.\n",
    "\n",
    "There is a simple relationship between the coefficient of determination and Pearson’s coefficient of correlation: R 2 = ρ 2. For example, if ρ is 0.8 or -0.8, R 2 = 0.64.\n",
    "Although ρ and R2 are often used to quantify the strength of a relationship, they are not easy to interpret in terms of predictive power. In my opinion, Std(res) is the best representation of the quality of prediction, especially if it is presented in relation to Std(ys).\n",
    "\n",
    "\n",
    "- testing a linear model \n",
    "One option is to test whether the apparent reduction in MSE is due to chance. In that case, the test statistic is R2 and the null hypothesis is that there is no relationship between the variables. \n",
    "\n",
    "In fact, because R 2 = ρ 2, a one-sided test of R2 is equivalent to a two-sided test of ρ. We’ve already done that test, and found p < 0.001, so we conclude that the apparent relationship between mother’s age and birth weight is statistically significant.\n",
    "\n",
    "Another approach is to test whether the apparent slope is due to chance. The null hy‐ pothesis is that the slope is actually zero; in that case we can model the birth weights as random variations around their mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Regression \n",
    "\n",
    "The goal of regression analysis is to describe the relationship between one set of vari‐ ables, called the dependent variables, and another set of variables, called independent or explanatory variables.\n",
    "\n",
    "In the previous chapter we used mother’s age as an explanatory variable to predict birth weight as a dependent variable. When there is only one dependent and one explanatory variable, that’s simple regression. In this chapter, we move on to multiple regression, with more than one explanatory variable. If there is more than one dependent variable, that’s multivariate regression.\n",
    "\n",
    "If the relationship between the dependent and explanatory variable is linear, that’s linear regression. For example, if the dependent variable is y and the explanatory variables are x1 and x2, we would write the following linear regression model:\n",
    "y = β0 + β1x1 + β2x2 + ε\n",
    "\n",
    "\n",
    "Given a sequence of values for y and sequences for x1 and x2, we can find the parameters, β0, β1, and β2, that minimize the sum of ε2. This process is called ordinary least squares. \n",
    "\n",
    "Because isfirst is a boolean, ols treats it as a categorical variable, which means that the values fall into categories, like True and False, and should not be treated as numbers. The estimated parameter is the effect on birth weight when isfirst is true, so the result, -0.125 lbs, is the difference in birth weight between first babies and others.\n",
    "\n",
    "**Nonlinear Relationships**\n",
    "Remembering that the contribution of agepreg might be non-linear, we might consider adding a variable to capture more of this relationship. One option is to create a column, agepreg2, that contains the squares of the ages:\n",
    "        live['agepreg2'] = live.agepreg**2\n",
    "        \n",
    "**Prediction**\n",
    "The next step is to sort the results and select the variables that yield the highest values of R2.\n",
    "\n",
    "- look at coefficients for predictive power \n",
    "\n",
    "\n",
    "**Logistic regression **\n",
    "\n",
    "In the previous examples, some of the explanatory variables were numerical and some categorical (including boolean). But the dependent variable was always numerical.\n",
    "Linear regression can be generalized to handle other kinds of dependent variables. If the dependent variable is boolean, the generalized model is called logistic regression. If the dependent variable is an integer count, it’s called Poisson regression.\n",
    "\n",
    "If you encode the dependent variable numerically, for example 0 for a girl and 1 for a boy, you could apply ordinary least squares, but there would be problems. The linear model might be something like this:\n",
    "y = β0 + β1x1 + β2x2 + ε\n",
    "Where y is the dependent variable, and x1 and x2 are explanatory variables. Then we\n",
    "could find the parameters that minimize the residuals.\n",
    "The problem with this approach is that it produces predictions that are hard to interpret. Given estimated parameters and values for x1 and x2, the model might predict y = 0.5, but the only meaningful values of y are 0 and 1.\n",
    "It is tempting to interpret a result like that as a probability; for example, we might say that a respondent with particular values of x1 and x2 has a 50% chance of having a boy. But it is also possible for this model to predict y = 1.1 or y = - 0.1, and those are not valid probabilities.\n",
    "Logistic regression avoids this problem by expressing predictions in terms of odds rather than probabilities. If you are not familiar with odds, “odds in favor” of an event is the ratio of the probability it will occur to the probability that it will not.\n",
    "So if I think my team has a 75% chance of winning, I would say that the odds in their favor are three to one, because the chance of winning is three times the chance of losing.\n",
    "Odds and probabilities are different representations of the same information. Given a probability, you can compute the odds like this:\n",
    "o = p / (1-p)\n",
    "Given odds in favor, you can convert to probability like this:\n",
    "p = o / (o+1)\n",
    "Logistic regression is based on the following model:\n",
    "log o = β0 +β1x1 +β2x2 +ε\n",
    "Where o is the odds in favor of a particular outcome; in the example, o would be the\n",
    "odds of having a boy.\n",
    "Suppose we have estimated the parameters β0, β1, and β2 (I’ll explain how in a minute). And suppose we are given values for x1 and x2. We can compute the predicted value of\n",
    "log o, and then convert to a probability: o = np.exp(log_o)\n",
    "p = o / (o+1)\n",
    "So in the office pool scenario we could compute the predictive probability of having a boy. But how do we estimate the parameters?\\\n",
    "\n",
    "Estimating Parameters\n",
    "Unlike linear regression, logistic regression does not have a closed form solution, so it is solved by guessing an initial solution and improving it iteratively.\n",
    "The usual goal is to find the maximum-likelihood estimate (MLE), which is the set of parameters that maximizes the likelihood of the data.\n",
    "\n",
    "- Test accuracy of model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 12 Time Series Analysis\n",
    "\n",
    "**one place to start**\n",
    "\n",
    "The model seems like a good linear fit for the data; nevertheless, linear regression is not the most appropriate choice for this data:\n",
    "• First, there is no reason to expect the long-term trend to be a line or any other simple function. In general, prices are determined by supply and demand, both of which vary over time in unpredictable ways.\n",
    "• Second, the linear regression model gives equal weight to all data, recent and past. For purposes of prediction, we should probably give more weight to recent data.\n",
    "  150 | Chapter 12: Time Series Analysis\n",
    "• Finally, one of the assumptions of linear regression is that the residuals are uncor‐ related noise. With time series data, this assumption is often false because successive values are correlated.\n",
    "The next section presents an alternative that is more appropriate for time series data.\n",
    "\n",
    "**Moving Averages**\n",
    "\n",
    "Most time series analysis is based on the modeling assumption that the observed series is the sum of three components:\n",
    "Trend\n",
    "A smooth function that captures persistent changes\n",
    "Seasonality\n",
    "Periodic variation, possibly including daily, weekly, monthly, or yearly cycles\n",
    "Noise\n",
    "Random variation around the longterm trend\n",
    "\n",
    "A moving average divides the series into overlapping regions, called windows, and com‐ putes the average of the values in each window.\n",
    "One of the simplest moving averages is the rolling mean, which computes the mean of the values in each window. For example, if the window size is 3, the rolling mean com‐ putes the mean of values 0 through 2, 1 through 3, 2 through 4, etc.\n",
    "\n",
    "**Serial Correlation**\n",
    "As prices vary from day to day, you might expect to see patterns. If the price is high on Monday, you might expect it to be high for a few more days; and if it’s low, you might expect it to stay low. A pattern like this is called serial correlation, because each value is correlated with the next one in the series.\n",
    "\n",
    "To compute serial correlation, we can shift the time series by an interval called a lag, and then compute the correlation of the shifted series with the original:\n",
    "\n",
    "Autocorrelation\n",
    "If you think a series might have some serial correlation, but you don’t know which lags to test, you can test them all! The autocorrelation function is a function that maps from lag to the serial correlation with the given lag. “Autocorrelation” is another name for serial correlation, used more often when the lag is not 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 Survival Analysis\n",
    "\n",
    "Survival analysis is a way to describe how long things last. It is often used to study human lifetimes, but it also applies to “survival” of mechanical and electronic compo‐ nents, or more generally to intervals in time before an event.\n",
    "If someone you know has been diagnosed with a life-threatening disease, you might have seen a “5-year survival rate,” which is the probability of surviving five years after diagnosis. That estimate and related statistics are the result of survival analysis.\n",
    "\n",
    "Survival Curves\n",
    "The fundamental concept in survival analysis is the survival curve, S(t), which is a function that maps from a duration, t, to the probability of surviving longer than t. If you know the distribution of durations, or “lifetimes”, finding the survival curve is easy; it’s just the complement of the CDF:\n",
    "S(t) = 1 - CDF(t)\n",
    "where CDF (t ) is the probability of a lifetime less than or equal to t.\n",
    "\n",
    "Hazard Function\n",
    "From the survival function we can derive the hazard function; for pregnancy lengths, the hazard function maps from a time, t, to the fraction of pregnancies that continue until t and then end at t. \n",
    "\n",
    "\n",
    "\n",
    "Estimating Survival Curves\n",
    "If someone gives you the CDF of lifetimes, it is easy to compute the survival and hazard functions. But in many real-world scenarios, we can’t measure the distribution of life‐ times directly. We have to infer it.\n",
    "For example, suppose you are following a group of patients to see how long they survive after diagnosis. Not all patients are diagnosed on the same day, so at any point in time, some patients have survived longer than others. If some patients have died, we know their survival times. For patients who are still alive, we don’t know survival times, but we have a lower bound.\n",
    "\n",
    "If we wait until all patients are dead, we can compute the survival curve, but if we are evaluating the effectiveness of a new treatment, we can’t wait that long! We need a way to estimate survival curves using incomplete information.\n",
    "As a more cheerful example, I will use NSFG data to quantify how long respondents “survive” until they get married for the first time. The range of respondents’ ages is 14 to 44 years, so the dataset provides a snapshot of women at different stages in their lives.\n",
    "For women who have been married, the dataset includes the date of their first marriage and their age at the time. For women who have not been married, we know their age when interviewed, but have no way of knowing when or if they will get married.\n",
    "Since we know the age at first marriage for some women, it might be tempting to exclude the rest and compute the CDF of the known data. That is a bad idea. The result would be doubly misleading: (1) older women would be overrepresented, because they are more likely to be married when interviewed, and (2) married women would be over‐ represented! In fact, this analysis would lead to the conclusion that all women get mar‐ ried, which is obviously incorrect.\n",
    "\n",
    "**Kaplan-Meier Estimation**\n",
    "In this example it is not only desirable but necessary to include observations of unmar‐ ried women, which brings us to one of the central algorithms in survival analysis, Kaplan-Meier estimation.\n",
    "The general idea is that we can use the data to estimate the hazard function, then convert the hazard function to a survival function. To estimate the hazard function, we consider, for each age, (1) the number of women who got married at that age and (2) the number of women “at risk” of getting married, which includes all women who were not married at an earlier age.\n",
    "\n",
    "ohort Effects\n",
    "One of the challenges of survival analysis is that different parts of the estimated curve are based on different groups of respondents. The part of the curve at time t is based on respondents whose age was at least t when they were interviewed. So the leftmost part of the curve includes data from all respondents, but the rightmost part includes only the oldest respondents.\n",
    "\n",
    "Extrapolation\n",
    "The survival curve for the ’70s cohort ends at about age 38; for the ’80s cohort it ends at age 28, and for the ’90s cohort we hardly have any data at all.\n",
    "We can extrapolate these curves by “borrowing” data from the previous cohort. Haz‐ ardFunction provides a method, Extend, that copies the tail from another longer Haz‐ ardFunction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 Analytic Methods\n",
    "\n",
    "Central Limit Theorem\n",
    "As we saw in the previous sections, if we add values drawn from normal distributions, the distribution of the sum is normal. Most other distributions don’t have this property; if we add values drawn from other distributions, the sum does not generally have an analytic distribution.\n",
    "But if we add up n values from almost any distribution, the distribution of the sum converges to normal as n increases.\n",
    "More specifically, if the distribution of the values has mean and standard deviation μ and σ, the distribution of the sum is approximately  (nμ, nσ 2).\n",
    "This result is the Central Limit Theorem (CLT). It is one of the most useful tools for statistical analysis, but it comes with caveats:\n",
    "• The values have to be drawn independently. If they are correlated, the CLT doesn’t apply (although this is seldom a problem in practice).\n",
    " 186 | Chapter 14: Analytic Methods\n",
    "• The values have to come from the same distribution (although this requirement can be relaxed).\n",
    "• The values have to be drawn from a distribution with finite mean and variance. So most Pareto distributions are out.\n",
    "• The rate of convergence depends on the skewness of the distribution. Sums from an exponential distribution converge for small n. Sums from a lognormal distri‐ bution require larger sizes.\n",
    "The Central Limit Theorem explains the prevalence of normal distributions in the nat‐ ural world. Many characteristics of living things are affected by genetic and environ‐ mental factors whose effect is additive. The characteristics we measure are the sum of a large number of small effects, so their distribution tends to be normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
